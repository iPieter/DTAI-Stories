<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ProbLog | DTAI</title><link>https://dtai.cs.kuleuven.be/posts/tags/problog/</link><atom:link href="https://dtai.cs.kuleuven.be/posts/tags/problog/index.xml" rel="self" type="application/rss+xml"/><description>ProbLog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 03 Aug 2019 10:36:34 +0200</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>ProbLog</title><link>https://dtai.cs.kuleuven.be/posts/tags/problog/</link></image><item><title>DeepProbLog</title><link>https://dtai.cs.kuleuven.be/posts/post/robin-manhaeve/deepproblog/</link><pubDate>Sat, 03 Aug 2019 10:36:34 +0200</pubDate><guid>https://dtai.cs.kuleuven.be/posts/post/robin-manhaeve/deepproblog/</guid><description>&lt;h1 id="deepproblog">DeepProbLog&lt;/h1>
&lt;p>DeepProbLog is a neuro-symbolic framework that integrates the probabilistic logic programming language ProbLog with neural networks.&lt;/p>
&lt;p>The full paper can be found
&lt;a href="https://arxiv.org/abs/1907.08194" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>The main strengths of DeepProbLog are:&lt;/p>
&lt;ul>
&lt;li>It combines probabilistic reasoning, logical reasoning and the power of neural networks.&lt;/li>
&lt;li>It can train neural networks and learn probabilistic paramters from examples.&lt;/li>
&lt;li>We retain both logic and neural networks as edge cases.&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-deepproblog">What is DeepProbLog?&lt;/h2>
&lt;p>DeepProbLog is an extension of ProbLog that integrates neural networks through the concept of the neural predicate. It allows us to combine high-level logical reasoning with the sub-symbolic power of neural networks.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code>nn(mnist_classifier,[X],Y,[0..9]) :: digit(X,Y).
&lt;/code>&lt;/pre>
&lt;p>The neural predicate defined above declares that there&amp;rsquo;s a neural network that will take in an input X, and has at its output a probability distribution. This relation can be used in the remainder of the program using the digit relation.&lt;/p>
&lt;p>TODO: ADD figure for digit predicate distribution&lt;/p>
&lt;p>We could for example define the addition over two MNIST digits:&lt;/p>
&lt;pre>&lt;code>nn(mnist_classifier,[X],Y,[0..9]) :: digit(X,Y).
addition(X,Y,Z) :- digit(X,N1), digit(Y,N2), Z is N1+N2.
&lt;/code>&lt;/pre>
&lt;p>Where the addition relation now defines the probability distribution over the sum of the individual digits.&lt;/p>
&lt;p>TODO: add figure for addition predicate distribution&lt;/p>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;h4 id="mnist-addition">MNIST addition&lt;/h4>
&lt;p>We compared the MNIST addition example describe above with a convolutional neural network baseline.
The result shows that the inclusion of the logic allows the model to train quicker and achieve a higher accuracy. It&amp;rsquo;s also important to note that the neural network trained inside the DeepProbLog model can recognize single digits, whereas the convolutional baselines can only classify sums. The separation between the logic and neural aspects results in a more flexible model.
&lt;img src="mnist.png" alt="MNISTS result">&lt;/p>
&lt;h4 id="sketching">Sketching&lt;/h4>
&lt;p>We reimplemented the experiments from the Differentiable Forth paper. These use a sketching approach to learn the missing behaviour from partial programs using small neural modules.&lt;/p>
&lt;p>From the result we can see that we perform similar to the original (neural) model. For learning to sort lists, Differentiable Forth starts to struggle starting from length 4. This is due to the long program trace. DeepProbLog does not have this problem thanks to the fact that it can perform almost all of the program in the logic.
&lt;img src="d4.png" alt="Differentiable Forth result">&lt;/p></description></item></channel></rss>